{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "false-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "external-advisory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/big/BD_project\n",
      "2019-Oct.csv  local_part.csv  Recommender.ipynb\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "similar-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('e_comm.csv') # Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df.count(), len(df.columns))) # Number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema() # schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(rand()).show(10,False) # random sample from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "streaming-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=df.randomSplit([0.9,0.1]) # split dataset\n",
    "# test.write.format(\"csv\").save(\"test1.csv\")\n",
    "test.coalesce(1).write.csv('result.csv') # write to 1 file or \"$ cat output/p* > test1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sixth-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # visualizations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comparable-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('local_part.csv') # Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "actual-captain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4245648, 9)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns))) # Number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "automatic-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=df.randomSplit([0.7,0.3]) # split local dataset\n",
    "test.coalesce(1).write.csv('local_test.csv')\n",
    "train.coalesce(1).write.csv('local_train.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-january",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
